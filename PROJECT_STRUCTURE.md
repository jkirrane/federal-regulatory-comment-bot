# Project Structure Guide

Complete file structure for the Regulatory Comment Bot project.

## ðŸ“ Directory Structure

```
regulatory-comment-bot/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ daily.yml                    # GitHub Actions workflow (Phase 4)
â”‚
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ schema.sql                       # Database schema (Phase 1)
â”‚   â”œâ”€â”€ db.py                            # Database utilities (Phase 1)
â”‚   â””â”€â”€ comment_periods.db               # SQLite database (created by db.py, committed to git)
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py                      # Makes it a Python package
â”‚   â”œâ”€â”€ base.py                          # BaseScraper class (Phase 2)
â”‚   â”œâ”€â”€ regulations_gov.py               # Regulations.gov scraper (Phase 2)
â”‚   â”œâ”€â”€ federal_register.py              # Federal Register enrichment (Phase 2)
â”‚   â””â”€â”€ categorizer.py                   # Topic categorization (Phase 2)
â”‚
â”œâ”€â”€ bot/
â”‚   â”œâ”€â”€ __init__.py                      # Makes it a Python package
â”‚   â”œâ”€â”€ bluesky_poster.py                # Bluesky API wrapper (Phase 3)
â”‚   â””â”€â”€ post_periods.py                  # Posting logic (Phase 3)
â”‚
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ build.py                         # Static site generator (Phase 5)
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ index.html.template          # Homepage template
â”‚   â”‚   â”œâ”€â”€ period.html.template         # Individual period page template
â”‚   â”‚   â””â”€â”€ base.html.template           # Base template (header/footer)
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ styles.css                   # Main stylesheet
â”‚       â”œâ”€â”€ script.js                    # Client-side filtering
â”‚       â””â”€â”€ images/
â”‚           â””â”€â”€ logo.png                 # Optional logo
â”‚
â”œâ”€â”€ docs/                                # Generated website (GitHub Pages)
â”‚   â”œâ”€â”€ index.html                       # Generated by build.py
â”‚   â”œâ”€â”€ feed.xml                         # RSS feed
â”‚   â”œâ”€â”€ data.json                        # JSON API
â”‚   â”œâ”€â”€ styles.css                       # Copied from web/static/
â”‚   â””â”€â”€ script.js                        # Copied from web/static/
â”‚
â”œâ”€â”€ tests/                               # Optional: Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_database.py
â”‚   â”œâ”€â”€ test_scrapers.py
â”‚   â””â”€â”€ test_bot.py
â”‚
â”œâ”€â”€ .env                                 # Environment variables (NOT committed)
â”œâ”€â”€ .env.example                         # Template for .env (committed)
â”œâ”€â”€ .gitignore                           # Git ignore rules
â”‚
â”œâ”€â”€ README.md                            # Project overview
â”œâ”€â”€ AGENTS.md                            # AI assistant guide
â”œâ”€â”€ API_DOCUMENTATION.md                 # Complete API reference
â”œâ”€â”€ IMPLEMENTATION_GUIDE.md              # Step-by-step build guide
â”œâ”€â”€ LESSONS_FROM_COURT_BOT.md           # Architectural patterns
â”œâ”€â”€ REGULATORY_COMMENT_BOT_SPEC.md       # Full specification
â”œâ”€â”€ PROJECT_STRUCTURE.md                 # This file
â”‚
â””â”€â”€ requirements.txt                     # Python dependencies
```

## Ã°Å¸" File Purposes

### Core Documentation (Include These!)

**YES - Include these 4 files in your project:**

1. **REGULATORY_COMMENT_BOT_SPEC.md**
   - Complete project specification
   - Database schema
   - Bot behavior
   - Website features
   - *Essential reference for anyone working on the project*

2. **API_DOCUMENTATION.md**
   - Regulations.gov API reference
   - Federal Register API reference
   - Query examples
   - Rate limits
   - *Critical for scraper development*

3. **IMPLEMENTATION_GUIDE.md**
   - Step-by-step build instructions
   - Phase-by-phase breakdown
   - Testing strategies
   - Troubleshooting tips
   - *Invaluable for incremental development*

4. **LESSONS_FROM_COURT_BOT.md**
   - Proven architectural patterns
   - What worked/didn't work
   - Code examples
   - Best practices
   - *Saves you from reinventing the wheel*

**Additional Documentation:**

5. **AGENTS.md** - Guide for working with AI coding assistants
6. **README.md** - Project overview and quick start
7. **PROJECT_STRUCTURE.md** - This file (optional but helpful)

### Configuration Files

- **.env** - Your actual credentials (NEVER commit this)
- **.env.example** - Template showing what env vars are needed
- **.gitignore** - What not to commit to git
- **requirements.txt** - Python dependencies

### Database Files

- **schema.sql** - Version-controlled schema definition
- **db.py** - Python utilities for database operations
- **comment_periods.db** - The actual SQLite database
  - **IMPORTANT:** This IS committed to git (it's our persistence layer)
  - GitHub Actions relies on this persisting between runs

### Source Code

**Scrapers:**
- `base.py` - Shared scraping utilities
- `regulations_gov.py` - Main scraper
- `federal_register.py` - Enrichment scraper
- `categorizer.py` - Topic classification

**Bot:**
- `bluesky_poster.py` - Bluesky API wrapper
- `post_periods.py` - Post formatting and logic

**Website:**
- `build.py` - Static site generator
- `templates/` - HTML templates
- `static/` - CSS, JS, images

### Generated Files

**docs/** - This folder is generated by `web/build.py` and contains your GitHub Pages site. It IS committed to git so GitHub can serve it.

### Workflow

**.github/workflows/daily.yml** - Automates:
1. Scraping new comment periods
2. Posting to Bluesky
3. Building website
4. Committing database and docs/

## ðŸ”§ Development Workflow

### Initial Setup (Once)

```bash
# 1. Create project directory
mkdir regulatory-comment-bot
cd regulatory-comment-bot

# 2. Initialize git
git init

# 3. Copy these files to your project:
# - REGULATORY_COMMENT_BOT_SPEC.md
# - API_DOCUMENTATION.md
# - IMPLEMENTATION_GUIDE.md
# - LESSONS_FROM_COURT_BOT.md
# - AGENTS.md
# - README.md
# - .gitignore
# - .env.example
# - requirements.txt

# 4. Create directory structure
mkdir -p .github/workflows database scrapers bot web/templates web/static docs tests

# 5. Create __init__.py files
touch scrapers/__init__.py bot/__init__.py tests/__init__.py

# 6. Set up environment
cp .env.example .env
# Edit .env with your credentials

# 7. Install dependencies
pip install -r requirements.txt

# 8. First commit
git add .
git commit -m "Initial project structure"

# 9. Create GitHub repo and push
git remote add origin https://github.com/yourusername/regulatory-comment-bot.git
git push -u origin main
```

### Phase-by-Phase Development

**Phase 1: Database (Session 1)**
1. Create `database/schema.sql`
2. Create `database/db.py`
3. Test: `python -m database.db`
4. Commit: `git add database/ && git commit -m "Add database schema and utilities"`

**Phase 2: Scraper (Sessions 2-3)**
1. Create `scrapers/base.py`
2. Create `scrapers/regulations_gov.py`
3. Create `scrapers/federal_register.py`
4. Create `scrapers/categorizer.py`
5. Test: `python -m scrapers.regulations_gov --dry-run`
6. Commit: `git add scrapers/ && git commit -m "Add scrapers"`

**Phase 3: Bot (Sessions 4-5)**
1. Create `bot/bluesky_poster.py`
2. Create `bot/post_periods.py`
3. Test: `python -m bot.post_periods --dry-run`
4. Commit: `git add bot/ && git commit -m "Add Bluesky bot"`

**Phase 4: Automation (Session 6)**
1. Create `.github/workflows/daily.yml`
2. Add GitHub secrets
3. Test: Manual workflow trigger
4. Commit: `git add .github/ && git commit -m "Add GitHub Actions workflow"`

**Phase 5: Website (Sessions 7-8)**
1. Create `web/build.py`
2. Create templates and static files
3. Test: `python -m web.build && open docs/index.html`
4. Commit: `git add web/ docs/ && git commit -m "Add website generator"`
5. Enable GitHub Pages

## ðŸ“¦ What Gets Committed

### Always Commit
- âœ… All `.py` files
- âœ… All `.sql` files
- âœ… `database/comment_periods.db` (our persistence layer!)
- âœ… `docs/` folder (GitHub Pages)
- âœ… All documentation (`.md` files)
- âœ… `.gitignore`
- âœ… `.env.example`
- âœ… `requirements.txt`
- âœ… `.github/workflows/`

### Never Commit
- âŒ `.env` (contains secrets!)
- âŒ `__pycache__/`
- âŒ `venv/` or `env/`
- âŒ IDE files (`.vscode/`, `.idea/`)
- âŒ OS files (`.DS_Store`)

## ðŸŽ¯ Quick Reference

### Test a Component
```bash
python -m database.db              # Test database
python -m scrapers.regulations_gov --dry-run  # Test scraper
python -m bot.post_periods --dry-run          # Test bot
python -m web.build                           # Test website
```

### Inspect Database
```bash
sqlite3 database/comment_periods.db
> .schema
> SELECT COUNT(*) FROM comment_periods;
> SELECT * FROM comment_periods LIMIT 5;
> .quit
```

### Manual Workflow
```bash
# Full pipeline
python -m scrapers.regulations_gov
python -m bot.post_periods
python -m web.build

# Commit changes
git add database/ docs/
git commit -m "Update data - $(date)"
git push
```

## ðŸš€ Deployment

Once everything is working:

1. **GitHub Secrets:** Add to repo settings
   - `BLUESKY_HANDLE`
   - `BLUESKY_APP_PASSWORD`
   - `REGULATIONS_API_KEY`

2. **GitHub Pages:** Enable in repo settings
   - Source: Deploy from branch
   - Branch: `main`
   - Folder: `/docs`

3. **GitHub Actions:** Will run automatically
   - Daily at 9 AM ET
   - Or trigger manually

## ðŸ“š Using with VS Code Claude

When starting a new coding session:

1. Open VS Code with Claude extension
2. Open your project folder
3. In Claude chat, provide context:
   ```
   I'm building a regulatory comment bot. 
   
   Relevant files:
   - REGULATORY_COMMENT_BOT_SPEC.md
   - API_DOCUMENTATION.md
   - AGENTS.md
   
   I'm working on: [current phase/component]
   ```

4. Follow phase-by-phase prompts from AGENTS.md

5. Test after each component

6. Commit working code before moving to next component

---

**Ready to build!** ðŸ›–âœ¨
